import os
import json
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import OpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
import fitz
from sentence_transformers import SentenceTransformer, util
import torch
import faiss
import numpy as np

class AdvancedChatbot:
    def __init__(self, pdf_file='documento.pdf', log_file='chatbot_log.json'):
        self.setup_logging(log_file)
        load_dotenv()
        self.text_data = self.load_pdf(pdf_file)  
        self.setup_language_model()
        self.setup_conversation_memory()

        if self.text_data.strip():
            self.setup_semantic_search()  
        else:
            print("‚ö†Ô∏è No se ha cargado texto del PDF.")

    def setup_logging(self, log_file):
        self.log_file = log_file
        if not os.path.exists(log_file):
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump([], f)

    def log_interaccion(self, user_input, respuesta):
        log_entry = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "user_input": user_input,
            "respuesta": respuesta
        }
        try:
            with open(self.log_file, 'r+', encoding='utf-8') as f:
                logs = json.load(f)
                logs.append(log_entry)
                f.seek(0)
                json.dump(logs, f, indent=4)
        except Exception as e:
            print(f"Error al registrar log: {e}")

    def load_pdf(self, pdf_file):
        if not os.path.exists(pdf_file):
            print(f"‚ö†Ô∏è El archivo '{pdf_file}' no existe en la ruta especificada.")
            return ""

        try:
            doc = fitz.open(pdf_file)
            print(f"üìÑ Documento PDF cargado. P√°ginas: {doc.page_count}")
            text = "\n".join([page.get_text("text") for page in doc])
            if not text.strip():
                print("‚ö†Ô∏è El PDF est√° vac√≠o o no contiene texto legible.")
            return text
        except Exception as e:
            print(f"‚ö†Ô∏è Error cargando PDF: {e}")
            return ""

    def setup_language_model(self):
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("API key de OpenAI no encontrada")
            self.llm = OpenAI(temperature=0.7, openai_api_key=api_key, max_tokens=600)
            print("‚úÖ Modelo de lenguaje configurado correctamente.")
        except Exception as e:
            raise Exception(f"‚ùå Error al configurar modelo de lenguaje: {e}")

    def setup_conversation_memory(self):
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=False)
        print("‚úÖ Memoria de conversaci√≥n configurada.")

    def setup_semantic_search(self):
        if not self.text_data.strip():
            print("‚ö†Ô∏è No hay texto para procesar.")
            return

        print("üîç Configurando b√∫squeda sem√°ntica...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.modelo = SentenceTransformer("all-MiniLM-L6-v2", device=device)
        
        self.fragmentos_texto = self.dividir_texto(self.text_data)
        print(f"üìå Fragmentos generados: {len(self.fragmentos_texto)}")

        # Obtener embeddings
        self.embeddings_fragmentos = self.modelo.encode(self.fragmentos_texto, convert_to_tensor=True)
        embeddings_np = np.array(self.embeddings_fragmentos.cpu().detach().numpy())

        # Crear un √≠ndice FAISS simple con IndexFlatL2 (No es necesario entrenar este √≠ndice)
        self.index = faiss.IndexFlatL2(embeddings_np.shape[1])  # FAISS Index Flat L2
        self.index.add(embeddings_np)  # Agregar los embeddings al √≠ndice
        print("‚úÖ √çndice FAISS configurado para b√∫squeda sem√°ntica.")

    def dividir_texto(self, texto, max_tokens=1200): 
        palabras = texto.split()
        fragmentos = [" ".join(palabras[i:i+max_tokens]) for i in range(0, len(palabras), max_tokens)]
        return fragmentos

    def buscar_respuesta_semantica(self, user_input):
        if not hasattr(self, 'modelo'):
            print("‚ö†Ô∏è El modelo de b√∫squeda sem√°ntica no est√° inicializado.")
            return None
        
        # Obtener el embedding de la pregunta
        embedding_pregunta = self.modelo.encode(user_input, convert_to_tensor=True)

        # Realizar la b√∫squeda en el √≠ndice FAISS
        similitudes, indices = self.index.search(np.array([embedding_pregunta.cpu().detach().numpy()]), k=1)

        # Obtener el √≠ndice y la similitud de la mejor coincidencia
        indice_mejor = indices[0][0]
        max_similitud = similitudes[0][0]

        print(f"üîç √çndice de mejor coincidencia: {indice_mejor} | Similitud: {max_similitud:.4f}")

        # Umbral de similitud ajustado (puedes experimentar con valores m√°s altos)
        if max_similitud < 0.7:  # Aumenta el umbral para asegurarte de que sea relevante
            print("‚ö†Ô∏è La similitud es baja. Intentando con IA para generar respuesta.")
            return None

        fragmento_relevante = self.fragmentos_texto[indice_mejor]
        print(f"‚úÖ Fragmento seleccionado: {fragmento_relevante[:200]}...")

        respuesta_final = self.refinar_respuesta(fragmento_relevante, user_input)
        return respuesta_final

    def refinar_respuesta(self, fragmento, user_input):
        """
        Reformula la respuesta basada en el fragmento seleccionado y la pregunta del usuario.
        """
        prompt = f"""
        Eres Belito, un asistente virtual de Belo. Responde las preguntas de forma clara y amigable, bas√°ndote solo en el siguiente fragmento de texto extra√≠do de un PDF. Usa emojis y formato para hacer la respuesta m√°s atractiva y √∫til.

        Fragmento relevante: 
        {fragmento}

        Pregunta:
        {user_input}

        Respuesta:
        """
        return self.llm(prompt)

    def generar_respuesta_ia(self, user_input):
        try:
            respuesta = self.conversation.predict(input=user_input)
            print(f"ü§ñ Respuesta IA: {respuesta}")
            return respuesta
        except Exception as e:
            print(f"‚ùå Error generando respuesta IA: {e}")
            return "Lo siento, hubo un problema generando mi respuesta."

    def chat(self):
        print("ü§ñ ¬°Hola! Soy Belito, tu asistente virtual. Escribe 'salir' para terminar.")
        while True:
            try:
                user_input = input("T√∫: ").strip()
                if user_input.lower() in ['salir', 'exit', 'bye']:
                    print("ü§ñ ¬°Hasta luego!")
                    break

                respuesta = self.buscar_respuesta_semantica(user_input)
                if not respuesta:
                    respuesta = self.generar_respuesta_ia(user_input)

                print(f"ü§ñ {respuesta}")
                self.log_interaccion(user_input, respuesta)
            except KeyboardInterrupt:
                print("\nü§ñ Operaci√≥n cancelada.")
            except Exception as e:
                print(f"ü§ñ Error inesperado: {e}")

def main():
    try:
        bot = AdvancedChatbot()
        bot.chat()
    except Exception as e:
        print(f"Error al iniciar el chatbot: {e}")

if __name__ == "__main__":
    main()
