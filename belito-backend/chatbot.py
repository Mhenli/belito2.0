import os
import json
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import OpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
import torch
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

class AdvancedChatbot:
    def __init__(self, json_file='documento.json', log_file='chatbot_log.json'):
        self.setup_logging(log_file)
        load_dotenv()
        self.text_data = self.load_json(json_file)  
        self.setup_language_model()
        self.setup_conversation_memory()

        if self.text_data:
            self.setup_semantic_search()
        else:
            print("‚ö†Ô∏è No se ha cargado texto del JSON.")

    def setup_logging(self, log_file):
        self.log_file = log_file
        if not os.path.exists(log_file):
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump([], f)

    def load_json(self, json_file):
        if not os.path.exists(json_file):
            print(f"‚ö†Ô∏è El archivo '{json_file}' no existe.")
            return []

        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            if isinstance(data, list):
                self.articles = [{"title": item["article"], "content": item["content"]} for item in data]
            else:
                self.articles = [{"title": data["article"], "content": data["content"]}]

            print(f"‚úÖ Cargados {len(self.articles)} art√≠culos del JSON.")
            return self.articles
        except Exception as e:
            print(f"‚ö†Ô∏è Error cargando JSON: {e}")
            return []

    def setup_language_model(self):
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("API key de OpenAI no encontrada")
            self.llm = OpenAI(temperature=0.7, openai_api_key=api_key, max_tokens=600)
            print("‚úÖ Modelo de lenguaje configurado correctamente.")
        except Exception as e:
            raise Exception(f"‚ùå Error al configurar modelo de lenguaje: {e}")

    def setup_conversation_memory(self):
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=False)
        print("‚úÖ Memoria de conversaci√≥n configurada.")

    def setup_semantic_search(self):
        if not self.articles:
            print("‚ö†Ô∏è No hay art√≠culos para procesar.")
            return

        print("üîç Configurando b√∫squeda sem√°ntica...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.modelo = SentenceTransformer("all-MiniLM-L6-v2", device=device)
        self.fragmentos_texto = [art["content"] for art in self.articles]
        self.embeddings_fragmentos = self.modelo.encode(self.fragmentos_texto, convert_to_tensor=True)
        embeddings_np = np.array(self.embeddings_fragmentos.cpu().detach().numpy())
        self.index = faiss.IndexFlatL2(embeddings_np.shape[1])  
        self.index.add(embeddings_np)  
        print("‚úÖ √çndice FAISS configurado para b√∫squeda sem√°ntica.")

    def buscar_respuesta_semantica(self, user_input):
        if not hasattr(self, 'modelo'):
            print("‚ö†Ô∏è El modelo de b√∫squeda sem√°ntica no est√° inicializado.")
            return None

        embedding_pregunta = self.modelo.encode(user_input, convert_to_tensor=True)
        similitudes, indices = self.index.search(np.array([embedding_pregunta.cpu().detach().numpy()]), k=1)
        indice_mejor = indices[0][0]
        max_similitud = similitudes[0][0]

        print(f"üîç Mejor coincidencia: √≠ndice {indice_mejor} | Similitud: {max_similitud:.4f}")

        if max_similitud < 0.5:
            print("‚ö†Ô∏è Similitud baja. Usando IA para generar respuesta.")
            return None

        fragmento_relevante = self.fragmentos_texto[indice_mejor]
        print(f"‚úÖ Fragmento seleccionado: {fragmento_relevante[:200]}...")

        return self.refinar_respuesta(fragmento_relevante, user_input)

    def refinar_respuesta(self, fragmento, user_input):
        prompt = f"""
        Eres Belito, un asistente virtual de Belo. Responde de manera clara y amigable, usando emojis y formato atractivo.

        Fragmento relevante: 
        {fragmento}

        Pregunta:
        {user_input}

        Respuesta:
        """

        print("üß† Prompt enviado a OpenAI:\n", prompt)  # üëà esto te muestra si algo en el prompt est√° mal
        try:
            respuesta = self.llm.invoke(prompt)
            return respuesta.strip()  # üëà as√≠ elimin√°s espacios vac√≠os al principio y fin
        except Exception as e:
            print(f"‚ùå Error refinando respuesta: {e}")
        return "Lo siento, hubo un problema al generar la respuesta con la informaci√≥n disponible."

    def generar_respuesta_ia(self, user_input):
        try:
            respuesta = self.conversation.predict(input=user_input)
            print(f"ü§ñ Respuesta IA: {respuesta}")
            return respuesta
        except Exception as e:
            print(f"‚ùå Error generando respuesta IA: {e}")
            return "Lo siento, hubo un problema generando mi respuesta."

    def chat(self):
        print("ü§ñ ¬°Hola! Soy Belito, tu asistente virtual. Escribe 'salir' para terminar.")
        while True:
            try:
                user_input = input("T√∫: ").strip()
                if user_input.lower() in ['salir', 'exit', 'bye']:
                    print("ü§ñ ¬°Hasta luego!")
                    break

                respuesta = self.buscar_respuesta_semantica(user_input)
                if not respuesta:
                    respuesta = self.generar_respuesta_ia(user_input)

                print(f"ü§ñ {respuesta}")
                self.log_interaccion(user_input, respuesta)
            except KeyboardInterrupt:
                print("\nü§ñ Operaci√≥n cancelada.")
            except Exception as e:
                print(f"ü§ñ Error inesperado: {e}")

    def log_interaccion(self, user_input, respuesta):
        with open(self.log_file, 'r+', encoding='utf-8') as f:
            data = json.load(f)
            data.append({"fecha": datetime.now().isoformat(), "usuario": user_input, "respuesta": respuesta})
            f.seek(0)
            json.dump(data, f, indent=4, ensure_ascii=False)


def main():
    try:
        bot = AdvancedChatbot(json_file="documento.json")
        bot.chat()
    except Exception as e:
        print(f"Error al iniciar el chatbot: {e}")


if __name__ == "__main__":
    main()
